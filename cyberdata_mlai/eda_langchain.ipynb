{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from itertools import combinations\n",
    "\n",
    "# langchain imports\n",
    "from langchain.llms import Ollama, HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    "\n",
    "\n",
    "# traditional ML imports\n",
    "from scipy.stats import ttest_ind, zscore\n",
    "from summarytools import dfSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds .env file and loads the vars\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"Key not found\")\n",
    "openai_org = os.getenv(\"OPENAI_ORG\", \"Organization not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Gemini\": VertexAI(model=\"gemini-2.0-flash-001\"),\n",
    "    \"Palm\": VertexAI(model=\"chat-bison\"),\n",
    "    \"Mistral\": Ollama(model=\"mistral\"),\n",
    "    \"Gemma\": Ollama(model=\"gemma\"),\n",
    "    # TODO: check if llama3 will be fast enough and substitute?\n",
    "    \"Llama\": Ollama(model=\"llama3\"),\n",
    "    \"Phi\": Ollama(model=\"phi\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "READ_FROM_PICKLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if READ_FROM_PICKLE:\n",
    "    malicious_df = pd.read_pickle(\"data/malicious_features_numeric.pkl\")\n",
    "    benign_df = pd.read_pickle(\"data/benign_features_numeric.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataset(df):\n",
    "    summary = f\"The dataset contains {len(df)} rows and it contains a network packet capture that was generated using Wireshark in an enterprize network. \"\n",
    "    summary += f\"Study the first 100 rows of the dataset and try to understand what it describes:\\n {df.head(100)}\"\n",
    "    summary += f\"Below are the summary statistics of the dataset\\n {df.describe()}.\"\n",
    "    # summary += f\"The correlation of the features of the dataset is given below \\n {df.corr()}.\"\n",
    "    with open(\"data/df_summary_mal.txt\", \"r\") as f:\n",
    "        summary += f\"A summary that includes statistics, histograms is given below \\n {f.read()}.\" \n",
    "    summary += \"Identify any anomalies in this time series dataset. Justify your conclusions based on known detections and security attacks.\" \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(malicious_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_data_summary = summarize_dataset(malicious_df)\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"malicious_data_summary\"],\n",
    "    template=\"Analyze this dataset and identify anomalies and trends:\\n{malicious_data_summary}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running analysis with {model_name}...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create chain and execute\n",
    "    response = model.invoke(\n",
    "        prompt_template.format(malicious_data_summary=malicious_data_summary)\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Store results\n",
    "    statistical_results[model_name] = {\n",
    "        \"response\": response,\n",
    "        \"time_taken\": round(end_time - start_time, 2),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name} Response:\\n{response}\\n\")\n",
    "    print(f\"Time Taken: {round(end_time - start_time, 2)} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing\n",
    "- Is the difference between two groups or variables statistically significant?\n",
    "- Use t-test to compare means of two groups\n",
    "  - assumes that data follows normal distribution\n",
    "- Types of variables\n",
    "  - dependent: the effect of a phenomenon. For example, how does number of HTTP requests mean that a network is compromised?\n",
    "  - independent: the cause. The number of HTTP requests affects whether a network is compromised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_df.pop(\"Payload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis_testing(df, col1, col2):\n",
    "    group1 = df[col1]\n",
    "    group2 = df[col2]\n",
    "    pvalue = ttest_ind(group1, group2)[1]\n",
    "    if pvalue < 0.05:\n",
    "        return \"The difference between {} and {} is statistically significant (p < 0.05)\".format(\n",
    "            col1, col2\n",
    "        )\n",
    "    else:\n",
    "        return \"The difference between {} and {} is not statistically significant (p >= 0.05)\".format(\n",
    "            col1, col2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_combinations(df):\n",
    "    df_columns = df.columns.tolist()\n",
    "    pairs = list(combinations(df_columns, 2))\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_hypotheses(df):\n",
    "    # perform hypothesis testing for all combinations of columns\n",
    "    all_column_pairs = get_column_combinations(malicious_df)\n",
    "    hypotheses = \"\"\n",
    "    for pair in all_column_pairs:\n",
    "        hypotheses += hypothesis_testing(df, pair[0], pair[1])\n",
    "        # ask model to explain\n",
    "    explain = f\"Below there is all the hypothesis testing performed with ttest for all the possible combinations of the features of the dataset. Extract logical conclusions based on the hypotheses testings. Is there a difference between two groups of variables that is statistically significant? Can you conclude if there are dependent or independent variables in the dataset? \\n ** Hypotheses Tests ** {hypotheses}\"\n",
    "    \n",
    "    return explain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses = all_hypotheses(malicious_df)\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"hypotheses\"],\n",
    "    template=\"Analyze this dataset based on the hypotheses tests and identify anomalies and trends:\\n{hypotheses}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running analysis with {model_name}...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create chain and execute\n",
    "    response = model.invoke(prompt_template.format(hypotheses=hypotheses))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Store results\n",
    "    hypotheses_results[model_name] = {\n",
    "        \"response\": response,\n",
    "        \"time_taken\": round(end_time - start_time, 2),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name} Response:\\n{response}\\n\")\n",
    "    print(f\"Time Taken: {round(end_time - start_time, 2)} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "- observation that significantly differs from others in a dataset\n",
    "- Causes\n",
    "  - measurement errors\n",
    "  - extreme rare values\n",
    "- significant impact in statistical analysis\n",
    "- measurements\n",
    "  - z-score: `(x - mean) / std_dev`\n",
    "  - IQR method: this method identifies outliers as observations that are below `Q1 - 1.5IQR` or above `Q3 + 1.5IQR`, where Q1 and Q3 are the first and third quartiles, and IQR is the interquartile range (the difference between Q3 and Q1).\n",
    "  - visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    zscores = np.abs(zscore(df[column]))\n",
    "    return df[zscores > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_outliers(df):\n",
    "    outliers = \"\"\n",
    "    for feature in df.columns:\n",
    "        outliers += f\"Outlier scores for {feature} are {detect_outliers_zscore(df, feature, threshold=3)}\"\n",
    "        # ask model to explain\n",
    "    explain = f\"Below there is all the outliers scores calculated using zscore, for all the possible combinations of the features of the dataset. Extract logical conclusions based on the outliers. Do you find any interesting observations that stand out in the dataset based on the outlier calculations? What are your conclusions? \\n ** Outlier Scores ** {outliers}\"\n",
    "\n",
    "    return explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = all_outliers(malicious_df)\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"outliers\"],\n",
    "    template=\"Analyze this dataset based on the outlier calculations and identify anomalies and trends:\\n{outliers}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running analysis with {model_name}...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create chain and execute\n",
    "    response = model.invoke(prompt_template.format(outliers=outliers))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Store results\n",
    "    outliers_results[model_name] = {\n",
    "        \"response\": response,\n",
    "        \"time_taken\": round(end_time - start_time, 2),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name} Response:\\n{response}\\n\")\n",
    "    print(f\"Time Taken: {round(end_time - start_time, 2)} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_summary = summarize_dataset(malicious_df)    \n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"eda_summary\"],\n",
    "    template=\"\"\" Given the following dataset summary statistics: \n",
    "    ### Summary statistics\n",
    "    {eda_summary}\n",
    "    Generate a Python script using Matplotlib and Seaborn to visualize:\n",
    "    1. A time series plot of value over time,\n",
    "    2. Highlight anomalies in the dataset,\n",
    "    3. Suggest interesting trends\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_visualizations = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running analysis with {model_name}...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create chain and execute\n",
    "    response = model.invoke(prompt_template.format(eda_summary=eda_summary))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Store results\n",
    "    model_visualizations[model_name] = {\n",
    "        \"response\": response,\n",
    "        \"time_taken\": round(end_time - start_time, 2),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name} Response:\\n{response}\\n\")\n",
    "    print(f\"Time Taken: {round(end_time - start_time, 2)} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_data_summary = summarize_dataset(benign_df)\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"malicious_data_summary\", \"benign_data_summary\"],\n",
    "    template=\"Analyze the two input datasets that were taken by a network tap using wireshark, based on the summaries given below: \\n Dataset1 Summary: {malicious_data_summary}\\n Dataset2 Summary: {benign_data_summary}.\\n What are the differences, what are the similarities between the two datasets? Can you identify if any of the two datasets exhibits malicious or benign behavior? Why? Give any other interesting observations that you extract from the two datasets.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running analysis with {model_name}...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create chain and execute\n",
    "    response = model.invoke(\n",
    "        prompt_template.format(\n",
    "            malicious_data_summary=malicious_data_summary,\n",
    "            benign_data_summary=benign_data_summary,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Store results\n",
    "    comparison_results[model_name] = {\n",
    "        \"response\": response,\n",
    "        \"time_taken\": round(end_time - start_time, 2),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{model_name} Response:\\n{response}\\n\")\n",
    "    print(f\"Time Taken: {round(end_time - start_time, 2)} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate EDA with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    # NOTE: you can only provide either criteria or evaluation_steps, and not both\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "        \"You should give more points to the responses that are based on the logical analysis of numerical results.\",\n",
    "        \"You should penalize responses that are lacking detailed explanations\",\n",
    "        \"You should penalize responses that are contradictory to the ground truth\",\n",
    "        \"Different numbers in the responses are OK, however contradicting opinions are not OK.\",\n",
    "    ],\n",
    "    evaluation_params=[\n",
    "        LLMTestCaseParams.INPUT,\n",
    "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "        LLMTestCaseParams.EXPECTED_OUTPUT,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_human_output = \"\"\"1. The source ports seem to have some unusually high frequencies in higher values and the destination ports exhibit high variability. This may indicate scanning.\n",
    "2. The payload indicates dns queries to domains that resemble IoT devices such as broadlink routers.\n",
    "3. The packet lengths show an interesting distribution of some unusually high values that may point to exfiltration or malware download. \n",
    "4. There is unusually high activity in UDP protocol that may be suspicious of unusual services.\n",
    "5. There is also some unusual high frequency in destination port 23 and 2323 that point to telnet and IoT telnet.\n",
    "6. Finally the interarrival is unusually short, and that may indicate Denial of Service attack.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses_human_output = \"\"\" Based on the hypothesis testing results:\n",
    "1. All pairs of variables seem unrelated to statistically significant differences.\n",
    "2. Logically, variables that should correlate are the bytes in/out and interarrival but there seems not to be any correlation.\n",
    "3. The results are inconclusive and additional analysis may be needed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_human_output = \"\"\" Based on the outliers calculations we reach the following conclusions:\n",
    "1. There are some destination port outliers that may indicate abnormal malicious behavior.\n",
    "2. There are outliers in packet size that again may point to unusual activity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_correctness = {} \n",
    "for llm_item in statistical_results.items():\n",
    "    test_case = LLMTestCase(\n",
    "        input=malicious_data_summary,\n",
    "        actual_output=llm_item[1][\"response\"],\n",
    "        expected_output=summary_human_output,\n",
    "    )\n",
    "\n",
    "    correctness_metric.measure(test_case)\n",
    "    correctness_dict = {}\n",
    "\n",
    "    print(f\"Model {llm_item[0]} response correctness: {correctness_metric.score}\")\n",
    "    print(correctness_metric.reason)\n",
    "    statistics_correctness[llm_item[0]] =  correctness_metric.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis_correctness = {}\n",
    "for llm_item in hypotheses_results.items():\n",
    "    test_case = LLMTestCase(\n",
    "        input=malicious_data_summary,\n",
    "        actual_output=llm_item[1][\"response\"],\n",
    "        expected_output=hypotheses_human_output,\n",
    "    )\n",
    "\n",
    "    correctness_metric.measure(test_case)\n",
    "    print(f\"Model {llm_item[0]} response correctness: {correctness_metric.score}\")\n",
    "    print(correctness_metric.reason)\n",
    "    hypothesis_correctness[llm_item[0]]  = correctness_metric.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_correctness = {}\n",
    "for llm_item in outliers_results.items():\n",
    "    test_case = LLMTestCase(\n",
    "        input=malicious_data_summary,\n",
    "        actual_output=llm_item[1][\"response\"],\n",
    "        expected_output=outliers_human_output,\n",
    "    )\n",
    "\n",
    "    correctness_metric.measure(test_case)\n",
    "    print(f\"Model {llm_item[0]} response correctness: {correctness_metric.score}\")\n",
    "    print(correctness_metric.reason)\n",
    "    outlier_correctness[llm_item[0]] = correctness_metric.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "for model_name, score in statistics_correctness.items():\n",
    "    score_dict = {}\n",
    "    score_dict[model_name] = {\n",
    "        \"statistics_correctness\": score,\n",
    "        \"hypothesis_correctness\": hypothesis_correctness[model_name],\n",
    "        \"outlier_correctness\": outlier_correctness[model_name],\n",
    "    }\n",
    "    all_scores.append(score_dict)\n",
    "\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to long-format DataFrame\n",
    "all_scores_df_long = pd.DataFrame(\n",
    "    [\n",
    "        {\"Model\": list(item.keys())[0], \"Metric\": metric, \"Score\": value}\n",
    "        for item in all_scores\n",
    "        for metric, value in list(item.values())[0].items()\n",
    "    ]\n",
    ")\n",
    "all_scores_df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Metric\", y=\"Score\", hue=\"Model\", data=all_scores_df_long)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Model Scores Across Different Metrics\", fontsize=16)\n",
    "plt.xlabel(\"Metrics\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
