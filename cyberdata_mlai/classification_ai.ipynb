{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "\n",
    "# scikit llm imports\n",
    "from skllm.config import SKLLMConfig\n",
    "from skllm.models.gpt.classification.zero_shot import ZeroShotGPTClassifier\n",
    "from skllm.models.gpt.classification.few_shot import FewShotGPTClassifier\n",
    "from skllm.models.vertex.classification.zero_shot import ZeroShotVertexClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds .env file and loads the vars\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", \"Key not found\")\n",
    "openai_org = os.getenv(\"OPENAI_ORG\", \"Organization not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this is set to true, we use old data and pickle files that have been created\n",
    "READ_FROM_PICKLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if READ_FROM_PICKLE:\n",
    "    malicious_df = pd.read_pickle(\"data/malicious_features_numeric.pkl\")\n",
    "    benign_df = pd.read_pickle(\"data/benign_features_numeric.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels, 0 for benign, 1 for malicious\n",
    "malicious_df[\"label\"] = 1 \n",
    "benign_df[\"label\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything is text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_df[\"embedding\"] = malicious_df.apply(lambda row: ','.join(row.astype(str)), axis=1)\n",
    "benign_df[\"embedding\"] = benign_df.apply(lambda row: ','.join(row.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([malicious_df[[\"embedding\", \"label\"]], benign_df[[\"embedding\", \"label\"]]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your data loaded into a Pandas DataFrame called 'data'\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = data[data[\"label\"] == 1]\n",
    "minority_class = data[data[\"label\"] == 0]\n",
    "\n",
    "# Downsample majority class\n",
    "downsampled_majority = resample(\n",
    "    majority_class,\n",
    "    replace=False,  # sample without replacement\n",
    "    n_samples=len(minority_class),  # to match minority class\n",
    "    random_state=42,\n",
    ")  # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "downsampled_data = pd.concat([downsampled_majority, minority_class])\n",
    "\n",
    "# Shuffle the data\n",
    "downsampled_data = downsampled_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Now downsampled_data contains your downsampled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a subset because this takes loooong!\n",
    "data_size = 2000\n",
    "X_llm = data[\"embedding\"].head(data_size)\n",
    "y_llm = data[\"label\"].head(data_size)\n",
    "X_test = data[\"embedding\"].tail(int(data_size / 10))\n",
    "y_test = data[\"label\"].tail(int(data_size / 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKLLMConfig.set_openai_key(openai_api_key)\n",
    "SKLLMConfig.set_openai_org(openai_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_metrics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeroShotClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = ZeroShotGPTClassifier(openai_model=\"gpt-3.5-turbo\")\n",
    "# clf.fit(X_llm, y_llm)\n",
    "# labels = clf.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, labels)\n",
    "# prf = precision_recall_fscore_support(y_test, labels)\n",
    "# metrics_dict = {\n",
    "#     \"gpt-3.5-turbo-zero-shot\": {\n",
    "#         \"Accuracy\": accuracy,\n",
    "#         \"Precision\": float(prf[0][1]),\n",
    "#         \"Recall\": float(prf[1][1]),\n",
    "#         \"F1\": float(prf[2][1]),\n",
    "#     }\n",
    "# }\n",
    "# all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = ZeroShotGPTClassifier(openai_model=\"gpt-4o\")\n",
    "# clf.fit(X_llm, y_llm)\n",
    "# labels = clf.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, labels)\n",
    "# prf = precision_recall_fscore_support(y_test, labels)\n",
    "# metrics_dict = {\n",
    "#     \"gpt-4o-zero-shot\": {\n",
    "#         \"Accuracy\": accuracy,\n",
    "#         \"Precision\": float(prf[0][1]),\n",
    "#         \"Recall\": float(prf[1][1]),\n",
    "#         \"F1\": float(prf[2][1]),\n",
    "#     }\n",
    "# }\n",
    "# all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FewShotClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = FewShotGPTClassifier(openai_model=\"gpt-3.5-turbo\")\n",
    "# clf.fit(X_llm, y_llm)\n",
    "# labels = clf.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, labels)\n",
    "# prf = precision_recall_fscore_support(y_test, labels)\n",
    "# metrics_dict = {\n",
    "#     \"gpt-3.5-turbo-few-shot\": {\n",
    "#         \"Accuracy\": accuracy,\n",
    "#         \"Precision\": float(prf[0][1]),\n",
    "#         \"Recall\": float(prf[1][1]),\n",
    "#         \"F1\": float(prf[2][1]),\n",
    "#     }\n",
    "# }\n",
    "# all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = FewShotGPTClassifier(openai_model=\"gpt-4o\")\n",
    "# clf.fit(X_llm, y_llm)\n",
    "# labels = clf.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, labels)\n",
    "# prf = precision_recall_fscore_support(y_test, labels)\n",
    "# metrics_dict = {\n",
    "#     \"gpt-4o-few-shot\": {\n",
    "#         \"Accuracy\": accuracy,\n",
    "#         \"Precision\": float(prf[0][1]),\n",
    "#         \"Recall\": float(prf[1][1]),\n",
    "#         \"F1\": float(prf[2][1]),\n",
    "#     }\n",
    "# }\n",
    "# all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VertexAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = []\n",
    "zero_shot_metrics = []\n",
    "few_shot_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKLLMConfig.set_google_project(\"expel-engineering-internal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ZeroShotVertexClassifier(model=\"gemini-2.0-flash-001\")\n",
    "clf.fit(X_llm, y_llm)\n",
    "labels = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, labels)\n",
    "prf = precision_recall_fscore_support(y_test, labels)\n",
    "metrics_dict = {\n",
    "    \"Gemini-zero-shot\": {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": float(prf[0][1]),\n",
    "        \"Recall\": float(prf[1][1]),\n",
    "        \"F1\": float(prf[2][1]),\n",
    "    }\n",
    "}\n",
    "zero_shot_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ZeroShotVertexClassifier(model=\"chat-bison\")\n",
    "clf.fit(X_llm, y_llm)\n",
    "labels = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, labels)\n",
    "prf = precision_recall_fscore_support(y_test, labels)\n",
    "metrics_dict = {\n",
    "    \"Palm-zero-shot\": {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": float(prf[0][1]),\n",
    "        \"Recall\": float(prf[1][1]),\n",
    "        \"F1\": float(prf[2][1]),\n",
    "    }\n",
    "}\n",
    "zero_shot_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skollama.models.ollama.classification.zero_shot import ZeroShotOllamaClassifier\n",
    "from skollama.models.ollama.classification.few_shot import FewShotOllamaClassifier\n",
    "\n",
    "clf = ZeroShotOllamaClassifier(model=\"llama3\")\n",
    "clf.fit(X_llm, y_llm)\n",
    "labels = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, labels)\n",
    "prf = precision_recall_fscore_support(y_test, labels)\n",
    "metrics_dict = {\n",
    "    \"LLama-zero-shot\": {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": float(prf[0][1]),\n",
    "        \"Recall\": float(prf[1][1]),\n",
    "        \"F1\": float(prf[2][1]),\n",
    "    }\n",
    "}\n",
    "zero_shot_metrics.append(metrics_dict)\n",
    "all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = FewShotOllamaClassifier(model=\"llama3\")\n",
    "clf.fit(X_llm, y_llm)\n",
    "labels = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, labels)\n",
    "prf = precision_recall_fscore_support(y_test, labels)\n",
    "metrics_dict = {\n",
    "    \"LLama-few-shot\": {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": float(prf[0][1]),\n",
    "        \"Recall\": float(prf[1][1]),\n",
    "        \"F1\": float(prf[2][1]),\n",
    "    }\n",
    "}\n",
    "few_shot_metrics.append(metrics_dict)\n",
    "all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ZeroShotOllamaClassifier(model=\"mistral\")\n",
    "clf.fit(X_llm, y_llm)\n",
    "labels = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, labels)\n",
    "prf = precision_recall_fscore_support(y_test, labels)\n",
    "metrics_dict = {\n",
    "    \"Mistral-zero-shot\": {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": float(prf[0][1]),\n",
    "        \"Recall\": float(prf[1][1]),\n",
    "        \"F1\": float(prf[2][1]),\n",
    "    }\n",
    "}\n",
    "zero_shot_metrics.append(metrics_dict)\n",
    "all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = FewShotOllamaClassifier(model=\"mistral\")\n",
    "clf.fit(X_llm, y_llm)\n",
    "labels = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, labels)\n",
    "prf = precision_recall_fscore_support(y_test, labels)\n",
    "metrics_dict = {\n",
    "    \"Mistral-few-shot\": {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": float(prf[0][1]),\n",
    "        \"Recall\": float(prf[1][1]),\n",
    "        \"F1\": float(prf[2][1]),\n",
    "    }\n",
    "}\n",
    "few_shot_metrics.append(metrics_dict)\n",
    "all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ZeroShotOllamaClassifier(model=\"gemma\")\n",
    "clf.fit(X_llm, y_llm)\n",
    "labels = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, labels)\n",
    "prf = precision_recall_fscore_support(y_test, labels)\n",
    "metrics_dict = {\n",
    "    \"Gemma-zero-shot\": {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": float(prf[0][1]),\n",
    "        \"Recall\": float(prf[1][1]),\n",
    "        \"F1\": float(prf[2][1]),\n",
    "    }\n",
    "}\n",
    "zero_shot_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = FewShotOllamaClassifier(model=\"gemma\")\n",
    "# clf.fit(X_llm, y_llm)\n",
    "# labels = clf.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, labels)\n",
    "# prf = precision_recall_fscore_support(y_test, labels)\n",
    "# metrics_dict = {\n",
    "#     \"Gemma-few-shot\": {\n",
    "#         \"Accuracy\": accuracy,\n",
    "#         \"Precision\": float(prf[0][1]),\n",
    "#         \"Recall\": float(prf[1][1]),\n",
    "#         \"F1\": float(prf[2][1]),\n",
    "#     }\n",
    "# }\n",
    "# all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ZeroShotOllamaClassifier(model=\"phi\")\n",
    "clf.fit(X_llm, y_llm)\n",
    "labels = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, labels)\n",
    "prf = precision_recall_fscore_support(y_test, labels)\n",
    "metrics_dict = {\n",
    "    \"Phi-zero-shot\": {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": float(prf[0][1]),\n",
    "        \"Recall\": float(prf[1][1]),\n",
    "        \"F1\": float(prf[2][1]),\n",
    "    }\n",
    "}\n",
    "zero_shot_metrics.append(metrics_dict)\n",
    "all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = FewShotOllamaClassifier(model=\"phi\")\n",
    "clf.fit(X_llm, y_llm)\n",
    "labels = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, labels)\n",
    "prf = precision_recall_fscore_support(y_test, labels)\n",
    "metrics_dict = {\n",
    "    \"Phi-few-shot\": {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": float(prf[0][1]),\n",
    "        \"Recall\": float(prf[1][1]),\n",
    "        \"F1\": float(prf[2][1]),\n",
    "    }\n",
    "}\n",
    "few_shot_metrics.append(metrics_dict)\n",
    "all_metrics.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to long-format DataFrame\n",
    "all_scores_df_long = pd.DataFrame(\n",
    "    [\n",
    "        {\"Model\": list(item.keys())[0], \"Metric\": metric, \"Score\": value}\n",
    "        for item in all_metrics \n",
    "        for metric, value in list(item.values())[0].items()\n",
    "    ]\n",
    ")\n",
    "all_scores_df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Metric\", y=\"Score\", hue=\"Model\", data=all_scores_df_long)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Classification Evaluation Zero Shot vs Few Shot.\", fontsize=16)\n",
    "plt.xlabel(\"Metrics\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to long-format DataFrame\n",
    "zero_scores_df_long = pd.DataFrame(\n",
    "    [\n",
    "        {\"Model\": list(item.keys())[0], \"Metric\": metric, \"Score\": value}\n",
    "        for item in zero_shot_metrics\n",
    "        for metric, value in list(item.values())[0].items()\n",
    "    ]\n",
    ")\n",
    "zero_scores_df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Metric\", y=\"Score\", hue=\"Model\", data=zero_scores_df_long)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Classification Evaluation for Different Models with Zero Shot.\", fontsize=16)\n",
    "plt.xlabel(\"Metrics\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to long-format DataFrame\n",
    "few_scores_df_long = pd.DataFrame(\n",
    "    [\n",
    "        {\"Model\": list(item.keys())[0], \"Metric\": metric, \"Score\": value}\n",
    "        for item in few_shot_metrics\n",
    "        for metric, value in list(item.values())[0].items()\n",
    "    ]\n",
    ")\n",
    "zero_scores_df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Metric\", y=\"Score\", hue=\"Model\", data=few_scores_df_long)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Classification Evaluation for Different Models with Few Shot.\", fontsize=16)\n",
    "plt.xlabel(\"Metrics\", fontsize=12)\n",
    "plt.ylabel(\"Score\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skllm.models.gpt.classification.tunable import GPTClassifier\n",
    "\n",
    "clf = GPTClassifier(\n",
    "    base_model=\"gpt-3.5-turbo-0613\",\n",
    "    n_epochs=None,  # int or None. When None, will be determined automatically by OpenAI\n",
    "    default_label=\"Random\",  # optional\n",
    ")\n",
    "\n",
    "clf.fit(X_llm, y_llm)  # y_train is a list of labels\n",
    "labels = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy_score(y_test, labels):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
