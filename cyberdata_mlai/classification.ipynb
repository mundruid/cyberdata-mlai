{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# scikit learn imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_features = pd.read_pickle(\"data/malicious_features.pkl\")\n",
    "benign_features = pd.read_pickle(\"data/benign_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_features.pop(\"Payload\")\n",
    "benign_features.pop(\"Payload\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling\n",
    "We label and concatenate benign and malicious before one-hot because there are different ports in each dataset and concatenating the two after one hot will not work with different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels, 0 for benign, 1 for malicious\n",
    "malicious_features[\"Labels\"] = 1\n",
    "benign_features[\"Labels\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes\n",
    "data_df = pd.concat(\n",
    "    [malicious_features, benign_features], ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize the data. TODO: is this needed??\n",
    "data_df = shuffle(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling\n",
    "Balancing the class that has the most data points, in our case the malicious class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your data loaded into a Pandas DataFrame called 'data'\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = data_df[data_df[\"Labels\"] == 1]\n",
    "minority_class = data_df[data_df[\"Labels\"] == 0]\n",
    "\n",
    "# Downsample majority class\n",
    "downsampled_majority = resample(\n",
    "    majority_class,\n",
    "    replace=False,  # sample without replacement\n",
    "    n_samples=len(minority_class),  # to match minority class\n",
    "    random_state=42,\n",
    ")  # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "downsampled_data = pd.concat([downsampled_majority, minority_class])\n",
    "\n",
    "# Shuffle the data\n",
    "downsampled_data = downsampled_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Now downsampled_data contains your downsampled dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "- We use some characteristic models for demonstration purposes. Models need to be tuned by exploring their hyperparameters and compared in order to pick the best one for the application.\n",
    "  - xgboost\n",
    "  - NN\n",
    "  - k-NN\n",
    "  - Random Forest\n",
    "- k-fold cross validation\n",
    "  - split the data in k parts\n",
    "  - use one part for testing, the other k-1 parts for training the model\n",
    "  - repeat for all the combinations of k parts using a different one for testing, the other k-1 for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after feature engineering, drop all the columns that are unnecessary\n",
    "downsampled_data = downsampled_data.drop([\"Source Port\", \"Destination Port\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the features and labels in separate dataframes\n",
    "X = downsampled_data.drop(\"Labels\", axis=1)\n",
    "y = downsampled_data[\"Labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to array\n",
    "data_array = downsampled_data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "We picked three characteristic models:\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=2, max_depth=2, learning_rate=1, objective=\"binary:logistic\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "## Metrics\n",
    "We compare all models using the following metrics:\n",
    "- accuracy: Accuracy is the proportion of correct predictions to the total number of predictions.\n",
    "- precision: Precision measures the proportion of true positives to the total predicted positives.\n",
    "- recall: Recall (also known as sensitivity or true positive rate) measures the proportion of true positives to the total actual positives.\n",
    "- f1 score\n",
    "\n",
    "Note: we use the models out of box without fine tuning them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_trained = train_model(decision_tree, X_train, y_train)\n",
    "dt_accuracy, dt_precision, dt_recall, dt_f1 = evaluate_model(decision_tree, X_test, y_test)\n",
    "print(\"Accuracy:\", dt_accuracy)\n",
    "print(\"Precision:\", dt_precision)\n",
    "print(\"Recall:\", dt_recall)\n",
    "print(\"F1 Score:\", dt_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_trained = train_model(rf, X_train, y_train)\n",
    "rf_accuracy, rf_precision, rf_recall, rf_f1 = evaluate_model(\n",
    "    rf, X_test, y_test\n",
    ")\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "print(\"Precision:\", rf_precision)\n",
    "print(\"Recall:\", rf_recall)\n",
    "print(\"F1 Score:\", rf_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_trained = train_model(xgb, X_train, y_train)\n",
    "xgb_accuracy, xgb_precision, xgb_recall, xgb_f1 = evaluate_model(xgb, X_test, y_test)\n",
    "print(\"Accuracy:\", xgb_accuracy)\n",
    "print(\"Precision:\", xgb_precision)\n",
    "print(\"Recall:\", xgb_recall)\n",
    "print(\"F1 Score:\", xgb_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "\n",
    "A confusion matrix is a table that visualizes the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model, X, y):\n",
    "    # Initialize an empty confusion matrix\n",
    "    cm = np.zeros((len(np.unique(y)), len(np.unique(y))), dtype=int)\n",
    "\n",
    "    # Perform k-fold cross-validation and accumulate the confusion matrix\n",
    "    predictions = cross_val_predict(model, X, y, cv=5)  # Change cv value as needed\n",
    "    cm += confusion_matrix(y, predictions)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(decision_tree, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_confusion_matrix(xgb, X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve\n",
    "\n",
    "Use a ROC curve to visualize the model evaluation:\n",
    "- The X-axis indicates the False Positive Rate\n",
    "- The Y-axis indicates the True Positive Rate\n",
    "- The higher the area under the curve the better the model performance\n",
    "- The diagonal line indicates the performance of a random classifier\n",
    "- Performance at the top level corner indicates high TPR with low TPR for the same threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(model, X, y, n_folds=5):\n",
    "    # Initialize variables to store false positive rate and true positive rate for each fold\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    tpr_sum = 0\n",
    "\n",
    "    X = X.values\n",
    "    y = y.values\n",
    "\n",
    "    # Create k-fold cross-validation iterator\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for train, test in cv.split(X, y):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        tpr_sum += np.interp(mean_fpr, fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, alpha=0.3)\n",
    "\n",
    "    mean_tpr = tpr_sum / n_folds\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", label=\"Random Chance\")\n",
    "    plt.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        color=\"b\",\n",
    "        lw=2,\n",
    "        label=\"Mean ROC (AUC = {:.2f})\".format(auc(mean_fpr, mean_tpr)),\n",
    "    )\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(decision_tree, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(rf, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(xgb, X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
